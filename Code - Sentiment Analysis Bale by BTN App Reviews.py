# -*- coding: utf-8 -*-
"""TK2 Kelompok 6

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1191akBfZScccSXdX61W7OGk433GwZGxi

Kelompok 6
- Azarine Aisyah Ramadhani (2206051550)
- Haifa Marwa Saniyyah (2206048783)
- Nadira Eka Rahmaharva (2206051525)
- Yiesha Reyhani Ghozali (2206828115)
"""

# Import Library

!pip install -qq google-play-scraper
!pip install Sastrawi --q
!pip install openpyxl --q
!pip install transformers --q
!pip install tensorflow --q

import json
import pandas as pd
import gdown

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google_play_scraper import Sort, reviews, app
from datetime import datetime
from datetime import datetime

import nltk
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from transformers import BertTokenizer, TFBertForSequenceClassification, BertForSequenceClassification
import tensorflow as tf

# Viz
from wordcloud import WordCloud
from plotly import graph_objs as go
import plotly.express as px
import plotly.figure_factory as ff
from collections import Counter

# Inisialisasi
tokenizer = word_tokenize
lemmatizer = WordNetLemmatizer()

import string
import re

"""# Scraping Data"""

# Ambil data
result, continuation_token = reviews(
    'id.co.btn.mobilebanking.android',
    lang='id',
    sort=Sort.NEWEST,
    count=5000
)

# Tentukan batas tanggal dengan jam 13:00
batas_tanggal = datetime(2025, 4, 28, 13, 0)

# Filter hasil berdasarkan tanggal dan jam
filtered_reviews = [review for review in result if review['at'] <= batas_tanggal]

# Ambil 2500 data teratas dari filtered_reviews
results = filtered_reviews[:2500]

# Hasil Scraping
df = pd.DataFrame(results)
df.info()

df.tail()

df['score'].value_counts()

pd.set_option("display.max_colwidth", None)

# Cuplikan 50 ulasan pertama
df['content'].head(50)

"""# Pre Processing

## Pre Processing (1)
"""

# Info mengenai data
df.info()

# Kamus Alay (Indonesian Colloquial)
url = "https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv"
kamus_alay_df = pd.read_csv(url)
print(kamus_alay_df.head())

# Convert df into dictionary
kamus_alay = dict(zip(kamus_alay_df['slang'], kamus_alay_df['formal']))
print(list(kamus_alay.items())[:10])

# Fungsi untuk Pre Processing
def lowercase(review_text):
    return review_text.lower()

def clean_text(review_text):
    # Case folding
    review_text = lowercase(review_text)

    # Remove emoji
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        "]+", flags=re.UNICODE)
    cleaned_text = emoji_pattern.sub(r'', review_text)

    # Remove hashtags
    cleaned_text = re.sub(r'#\w+', '', cleaned_text)

    # Remove numbers
    cleaned_text = re.sub(r'\d+', ' ', cleaned_text)

    # Remove punctuation
    cleaned_text = cleaned_text.translate(
        str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    )

    # Remove superscript
    superscript_pattern = re.compile("["
        u"\U00002070"
        u"\U000000B9"
        u"\U000000B2-\U000000B3"
        u"\U00002074-\U00002079"
        u"\U0000207A-\U0000207E"
        u"\U0000200D"
        "]+", flags=re.UNICODE)
    cleaned_text = superscript_pattern.sub(r'', cleaned_text)

    # Normalized slang words
    words = cleaned_text.split()
    normalized_words = []
    for word in words:
        lower_word = word.lower()
        if lower_word in kamus_alay:
            normalized_words.append(kamus_alay[lower_word])
        else:
            normalized_words.append(word)
    cleaned_text = ' '.join(normalized_words)

    # Remove character repetition
    cleaned_text = re.sub(r'(.)\1+', r'\1', cleaned_text)

    # Remove word repetition
    cleaned_text = re.sub(r'\b(\w+)(?:\W\1\b)+', r'\1', cleaned_text, flags=re.IGNORECASE)

    # Remove extra whitespaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # Stopwords Removal (without tokenization)
    stop_words = stopwords.words('indonesian') + stopwords.words('english') + ["yg",
                "gak", "ngisi", "udah", "d", "sih", "nya", "srg", "utk", "byk", "gk", "ga", "aja", "tp", "udh", "engak", "gue",
                "gua", "gweh", "lu", "lw"]

    # Split, filter stopwords, and rejoin as a string
    words = cleaned_text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    cleaned_text = ' '.join(filtered_words)

    return cleaned_text

# Apply Preprocessing (1)
df['preprocessing'] = df['content'].apply(clean_text)

# Hasil Preprocessing
df[['content', 'preprocessing']]

df2 = df[['content', 'preprocessing','score']]

# Drop rows dengan missing value pada kolom 'content'
df2.dropna(subset=['preprocessing'], inplace=True)

# Drop baris yang duplikat
df2.drop_duplicates(subset=['preprocessing'], keep='first', inplace=True)

df2

"""## Preprocessing (2)"""

df_eda = df2.copy()
def preprocess_eda(text):
    # Convert text to lowercase for case-insensitive matching
    text = text.lower()

    # Remove specific words - expanded list with word boundaries
    text = re.sub(r'\b(aplikasi|btn|mobile|aplikasinya|bale|apk|banget|kali|ya|bank)\b', '', text)

    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    # Stopwords Removal (without tokenization)
    stop_words = stopwords.words('indonesian') + stopwords.words('english') + ["yg",
                "gak", "ngisi", "udah", "d", "sih", "nya", "srg", "utk", "byk", "gk", "ga", "aja", "tp", "udh", "engak", "gue",
                "gua", "gweh", "lu", "lw"]

    # Split, filter stopwords, and rejoin as a string
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    text = ' '.join(filtered_words)

    return text

df_eda['eda'] = df_eda['content'].apply(preprocess_eda)
df_eda.head()

"""## Preprocessing (3)"""

def clean_text(review_text):
    # Remove emoji
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        "]+", flags=re.UNICODE)
    cleaned_text = emoji_pattern.sub(r'', review_text)

    # Remove hashtags
    cleaned_text = re.sub(r'#\w+', '', cleaned_text)

    # Remove numbers
    cleaned_text = re.sub(r'\d+', ' ', cleaned_text)

    # Remove punctuation
    cleaned_text = cleaned_text.translate(
        str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    )

    # Remove superscript
    superscript_pattern = re.compile("["
        u"\U00002070"
        u"\U000000B9"
        u"\U000000B2-\U000000B3"
        u"\U00002074-\U00002079"
        u"\U0000207A-\U0000207E"
        u"\U0000200D"
        "]+", flags=re.UNICODE)
    cleaned_text = superscript_pattern.sub(r'', cleaned_text)

    # Normalized slang words
    words = cleaned_text.split()
    normalized_words = []
    for word in words:
        lower_word = word.lower()  # Just for checking against dictionary
        if lower_word in kamus_alay:
            normalized_words.append(kamus_alay[lower_word])
        else:
            normalized_words.append(word)  # Keep original case
    cleaned_text = ' '.join(normalized_words)

    # Remove character repetition
    cleaned_text = re.sub(r'(.)\1+', r'\1', cleaned_text)

    # Remove word repetition
    cleaned_text = re.sub(r'\b(\w+)(?:\W\1\b)+', r'\1', cleaned_text, flags=re.IGNORECASE)

    # Remove extra whitespaces
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text

# Apply Preprocessing
df3 = df.copy()
df3['preprocessing'] = df3['content'].apply(clean_text)
df3[['content', 'preprocessing']]

df3 = df3[['content', 'preprocessing','score']]

# Menghapus baris dengan nilai yang hilang di kolom 'preprocessing'
df3.dropna(subset=['preprocessing'], inplace=True)

# Menghapus baris yang hanya berisi spasi kosong
df3 = df3[df3['preprocessing'].str.strip() != '']

# Menghapus baris duplikat, menyimpan kemunculan pertama
df3.drop_duplicates(subset=['preprocessing'], keep='first', inplace=True)

df3

"""## Labelling"""

# Labelling berdasarkan skor
df3['sentiment'] = df3['score'].apply(lambda x: 'positif' if x >= 4 else 'negatif')
df3

# Export ke Excel
df3.to_excel('reviews_sentiment.xlsx', index=False)

"""### Validasi manual"""

# Load Data (Preprocessing 1)
!gdown 196t8WbXbydGnOp38mdd4kY0IxroREHzU
data1 = pd.read_csv('/content/status ulasan - Sheet1.csv')

data1 = data1[['content', 'sentiment']]
data1.head()

# Encode labels
data1['label'] = data1['sentiment'].replace({'negatif': 0, 'positif': 1})
data1

# Load Data (Preprocessing 3)
!gdown 16ANsWg8DwxnftxrEMc3LkpNwSUuj2vCH
data = pd.read_excel('/content/status ulasan.xlsx')

data = data[['preprocessing', 'sentiment']]
data.head()

# Encode label
data.rename(columns={'preprocessing': 'content'}, inplace=True)
data['label'] = data['sentiment'].replace({'negatif': 0, 'positif': 1})
data

"""# EDA

## Preprocessing (1)
"""

df_positif = data1[data1['sentiment'] == 'positif']
all_words_positif = ' '.join([twts for twts in df_positif['content']])
wordcloud_positif = WordCloud(width=500, height=300, random_state=21, max_font_size=110, background_color='white').generate(all_words_positif)

plt.imshow(wordcloud_positif, interpolation="bilinear")
plt.axis('off')
plt.title('Word Cloud dari Sentimen Positif')
plt.show()

df_positif = data1[data1['sentiment'] == 'negatif']
all_words_positif = ' '.join([twts for twts in df_positif['content']])
wordcloud_positif = WordCloud(width=500, height=300, random_state=21, max_font_size=110, background_color='white').generate(all_words_positif)

plt.imshow(wordcloud_positif, interpolation="bilinear")
plt.axis('off')
plt.title('Word Cloud dari Sentimen Negatif')
plt.show()

sentiment_counts = data1['sentiment'].value_counts()

# Colors
colors = ['#FFB6C1', '#ADD8E6']  # Light pink, light blue

# Create the pie chart
plt.figure(figsize=(5, 5))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.2f%%', startangle=90, colors=colors)
plt.title('Distribusi Sentimen')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""## Preprocessing (2)"""

df_eda = data.copy()

def preprocess_eda(text):
    # Convert text to lowercase for case-insensitive matching
    text = text.lower()

    # Remove specific words - expanded list with word boundaries
    text = re.sub(r'\b(aplikasi|btn|mobile|aplikasinya|bale|apk|banget|kali|ya|bank)\b', '', text)

    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    # Stopwords Removal (without tokenization)
    stop_words = stopwords.words('indonesian') + stopwords.words('english') + ["yg",
                "gak", "ngisi", "udah", "d", "sih", "nya", "srg", "utk", "byk", "gk", "ga", "aja", "tp", "udh", "engak", "gue",
                "gua", "gweh", "lu", "lw"]

    # Split, filter stopwords, and rejoin as a string
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    text = ' '.join(filtered_words)

    return text

df_eda['eda'] = df_eda['content'].apply(preprocess_eda)
df_eda.head()

df_positif = df_eda[df_eda['sentiment'] == 'positif']
all_words_positif = ' '.join([twts for twts in df_positif['eda']])
wordcloud_positif = WordCloud(width=500, height=300, random_state=21, max_font_size=110, background_color='white').generate(all_words_positif)

plt.imshow(wordcloud_positif, interpolation="bilinear")
plt.axis('off')
plt.title('Word Cloud dari Sentimen Positif')
plt.show()

df_positif = df_eda[df_eda['sentiment'] == 'negatif']
all_words_positif = ' '.join([twts for twts in df_positif['eda']])
wordcloud_positif = WordCloud(width=500, height=300, random_state=21, max_font_size=110, background_color='white').generate(all_words_positif)

plt.imshow(wordcloud_positif, interpolation="bilinear")
plt.axis('off')
plt.title('Word Cloud dari Sentimen Negatif')
plt.show()

sentiment_counts = data['sentiment'].value_counts()

# Colors
colors = ['#FFB6C1', '#ADD8E6']  # Light pink, light blue

# Create the pie chart
plt.figure(figsize=(5, 5))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.2f%%', startangle=90, colors=colors)
plt.title('Distribusi Sentimen')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""# Modelling

## Model 1
"""

# Split data
X = data1['content']
y = data1['label']

# Split dataset
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.3, random_state=25, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_val, y_val, test_size=0.5, random_state=25, stratify=y_val
)

print(f"Training set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

from transformers import AutoTokenizer

# Inisialisasi tokenizer
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

# Tokenisasi data
def tokenize_function(texts):
    return tokenizer(
        texts.tolist(),
        padding=True,
        truncation=True,
        max_length=100
    )

train_encodings = tokenize_function(X_train)
val_encodings = tokenize_function(X_val)
test_encodings = tokenize_function(X_test)

from torch.utils.data import Dataset # Import Dataset from torch.utils.data

class ReviewsDataset(Dataset):
    def __init__(self, encodings, label):
        self.encodings = encodings
        self.label = label

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        # Check if idx is a list (for batching) or an integer (single item)
        if isinstance(idx, list):
            # If idx is a list, create a batch of items
            item = {key: torch.tensor([val[i] for i in idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor([self.label[i] for i in idx])
        else:
            # If idx is an integer, get a single item
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor(self.label[idx])
        return item

train_dataset = ReviewsDataset(train_encodings, y_train.tolist())
val_dataset = ReviewsDataset(val_encodings, y_val.tolist())
test_dataset = ReviewsDataset(test_encodings, y_test.tolist())

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler
from transformers import AutoTokenizer
from transformers import BertConfig
from torch.optim import AdamW # Import AdamW from torch.optim
import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

config = BertConfig.from_pretrained("indobenchmark/indobert-base-p1", num_labels=2, seed = 25)

# Inisialisasi model
model = AutoModelForSequenceClassification.from_pretrained("indobenchmark/indobert-base-p1", config = config)

# Buat optimizer AdamW
optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)

# Scheduler learning rate
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=500,
    num_training_steps=3 * len(train_dataset) // 32
)

# Define compute_metrics function
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    learning_rate=3e-5,
    eval_strategy='epoch',
    save_strategy='epoch'
)

# Inisialisasi trainer dengan optimizer dan scheduler custom
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics, # Pass the defined function
    optimizers=(optimizer, lr_scheduler)
)

# Train model
trainer.train()

import matplotlib.pyplot as plt

log_history = trainer.state.log_history

# List untuk menyimpan data per epoch
train_epochs = []
train_loss = []

eval_epochs = []
eval_loss = []
eval_accuracy = []

# Ekstrak data dari log_history
for log in log_history:
    # Training loss per epoch
    if 'loss' in log and 'epoch' in log:
        train_epochs.append(log['epoch'])
        train_loss.append(log['loss'])
    # Evaluation metrics per epoch
    if 'eval_loss' in log and 'epoch' in log:
        eval_epochs.append(log['epoch'])
        eval_loss.append(log['eval_loss'])
    if 'eval_accuracy' in log and 'epoch' in log:
        eval_accuracy.append(log['eval_accuracy'])

# Plotting
plt.figure(figsize=(12, 5))

# Plot Loss (Training & Validation)
plt.subplot(1, 2, 1)
plt.plot(train_epochs, train_loss, label='Training Loss', marker='o')
plt.plot(eval_epochs, eval_loss, label='Validation Loss', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training & Validation Loss per Epoch')
plt.legend()

# Plot Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(eval_epochs, eval_accuracy, label='Validation Accuracy', color='green', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy per Epoch')
plt.legend()

plt.tight_layout()
plt.show()

# Prediksi pada data testing
predictions = trainer.predict(test_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)

# Nilai sebenarnya (ground truth)
y_true = predictions.label_ids

# Hitung test accuracy
test_accuracy = accuracy_score(y_true, y_pred)

# Tampilkan test accuracy
print(f"Test Accuracy: {test_accuracy}")

cr = classification_report(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print("=== Classification Report ===")
print(cr)
print("=== Confusion Matrix (===")
print(cm)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = ['Negative', 'Positive']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""## Model 2"""

data.head()

import os
import random
import numpy as np
import tensorflow as tf

def set_seed(seed=25):
    # Pengaturan untuk Python core
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)

    # Pengaturan untuk NumPy
    np.random.seed(seed)

    # Pengaturan untuk TensorFlow
    tf.random.set_seed(seed)

    # Pengaturan tambahan untuk TensorFlow
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'


    # Jika menggunakan GPU
    try:
        tf.config.experimental.set_memory_growth(
            tf.config.list_physical_devices('GPU')[0], True)
    except:
        pass

# Aplikasikan seed
set_seed(25)

# Jika menggunakan Keras, tambahkan ini:
from tensorflow import keras
keras.utils.set_random_seed(25)

# Split data
X = data['content']
y = data['label']

# Split dataset
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.3, random_state=25, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_val, y_val, test_size=0.5, random_state=25, stratify=y_val
)

print(f"Training set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

from transformers import AutoTokenizer

# Inisialisasi tokenizer
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

# Tokenisasi data
def tokenize_function(texts):
    return tokenizer(
        texts.tolist(),
        padding=True,
        truncation=True,
        max_length=100
    )

train_encodings = tokenize_function(X_train)
val_encodings = tokenize_function(X_val)
test_encodings = tokenize_function(X_test)

from torch.utils.data import Dataset # Import Dataset from torch.utils.data
import torch

class ReviewsDataset(Dataset):
    def __init__(self, encodings, label):
        self.encodings = encodings
        self.label = label

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        # Check if idx is a list (for batching) or an integer (single item)
        if isinstance(idx, list):
            # If idx is a list, create a batch of items
            item = {key: torch.tensor([val[i] for i in idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor([self.label[i] for i in idx])
        else:
            # If idx is an integer, get a single item
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor(self.label[idx])
        return item

train_dataset = ReviewsDataset(train_encodings, y_train.tolist())
val_dataset = ReviewsDataset(val_encodings, y_val.tolist())
test_dataset = ReviewsDataset(test_encodings, y_test.tolist())

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler, BertConfig
from torch.optim import AdamW
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Konfigurasi model
config = BertConfig.from_pretrained(
    "indobenchmark/indobert-base-p1",
    num_labels=2,
    seed=25
)

model = AutoModelForSequenceClassification.from_pretrained(
    "indobenchmark/indobert-base-p1",
    config=config
)

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# Parameter training
train_batch_size = 32
num_train_epochs = 5
train_dataset_size = len(train_dataset)
total_training_steps = (train_dataset_size // train_batch_size) * num_train_epochs

# Scheduler LR
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=500,
    num_training_steps=total_training_steps
)

# Fungsi evaluasi
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# TrainingArguments dengan evaluasi per step dan save per step
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-5,

    # Penting: evaluasi dan save per step, agar load_best_model_at_end bisa jalan
    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=10,
    save_steps=10,

    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True,

    # Optional: untuk menghindari terlalu banyak checkpoint
    save_total_limit=3
)

# Inisialisasi Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    optimizers=(optimizer, lr_scheduler)
)

model

# Train model
trainer.train()

import matplotlib.pyplot as plt

log_history = trainer.state.log_history

# List untuk menyimpan data
train_steps = []
train_loss = []

eval_steps = []
eval_loss = []
eval_accuracy = []

# Ekstrak data dari log_history
for log in log_history:
    # Training loss biasanya muncul saat step training
    if 'loss' in log and 'step' in log:
        train_steps.append(log['step'])
        train_loss.append(log['loss'])
    # Evaluation metrics muncul saat evaluasi
    if 'eval_loss' in log and 'step' in log:
        eval_steps.append(log['step'])
        eval_loss.append(log['eval_loss'])
    if 'eval_accuracy' in log and 'step' in log:
        eval_accuracy.append(log['eval_accuracy'])

# Plotting
plt.figure(figsize=(12, 5))

# Plot Loss (Training & Validation)
plt.subplot(1, 2, 1)
plt.plot(train_steps, train_loss, label='Training Loss')
plt.plot(eval_steps, eval_loss, label='Validation Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# Plot Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(eval_steps, eval_accuracy, label='Validation Accuracy', color='green')
plt.xlabel('Step')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Import Library Tambahan

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Prediksi pada Data Testing
predictions = trainer.predict(test_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids

# Menghitung dan Menampilkan Test Accuracy
test_accuracy = accuracy_score(y_true, y_pred)
print(f"Test Accuracy: {test_accuracy}")

# Classification Report

cr = classification_report(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print("=== Classification Report ===")
print(cr)
print("=== Confusion Matrix (===")
print(cm)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = ['Negative', 'Positive']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""## Model 3"""

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler, BertConfig
from torch.optim import AdamW
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Konfigurasi model
config = BertConfig.from_pretrained(
    "indobenchmark/indobert-base-p1",
    num_labels=2,
    seed=25
)

model = AutoModelForSequenceClassification.from_pretrained(
    "indobenchmark/indobert-base-p1",
    config=config
)

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# Parameter training
train_batch_size = 32
num_train_epochs = 7
train_dataset_size = len(train_dataset)
total_training_steps = (train_dataset_size // train_batch_size) * num_train_epochs

# Scheduler LR
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=500,
    num_training_steps=total_training_steps
)

# Fungsi evaluasi
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# TrainingArguments dengan evaluasi per step dan save per step
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-5,

    # Penting: evaluasi dan save per step, agar load_best_model_at_end bisa jalan
    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=10,
    save_steps=10,

    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True,

    # Optional: untuk menghindari terlalu banyak checkpoint
    save_total_limit=3
)

# Inisialisasi Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    optimizers=(optimizer, lr_scheduler)
)

# Train model
trainer.train()

import matplotlib.pyplot as plt

log_history = trainer.state.log_history

# List untuk menyimpan data
train_steps = []
train_loss = []

eval_steps = []
eval_loss = []
eval_accuracy = []

# Ekstrak data dari log_history
for log in log_history:
    # Training loss biasanya muncul saat step training
    if 'loss' in log and 'step' in log:
        train_steps.append(log['step'])
        train_loss.append(log['loss'])
    # Evaluation metrics muncul saat evaluasi
    if 'eval_loss' in log and 'step' in log:
        eval_steps.append(log['step'])
        eval_loss.append(log['eval_loss'])
    if 'eval_accuracy' in log and 'step' in log:
        eval_accuracy.append(log['eval_accuracy'])

# Plotting
plt.figure(figsize=(12, 5))

# Plot Loss (Training & Validation)
plt.subplot(1, 2, 1)
plt.plot(train_steps, train_loss, label='Training Loss')
plt.plot(eval_steps, eval_loss, label='Validation Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# Plot Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(eval_steps, eval_accuracy, label='Validation Accuracy', color='green')
plt.xlabel('Step')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Prediksi pada Data Testing
predictions = trainer.predict(test_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids

# Menghitung dan Menampilkan Test Accuracy
test_accuracy = accuracy_score(y_true, y_pred)
print(f"Test Accuracy: {test_accuracy}")

# Classification Report

cr = classification_report(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print("=== Classification Report ===")
print(cr)
print("=== Confusion Matrix (===")
print(cm)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = ['Negative', 'Positive']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""## Model 4"""

import os
import random
import numpy as np
import tensorflow as tf

def set_seed(seed=25):
    # Pengaturan untuk Python core
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)

    # Pengaturan untuk NumPy
    np.random.seed(seed)

    # Pengaturan untuk TensorFlow
    tf.random.set_seed(seed)

    # Pengaturan tambahan untuk TensorFlow
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'


    # Jika menggunakan GPU
    try:
        tf.config.experimental.set_memory_growth(
            tf.config.list_physical_devices('GPU')[0], True)
    except:
        pass

# Aplikasikan seed
set_seed(25)

# Jika menggunakan Keras, tambahkan ini:
from tensorflow import keras
keras.utils.set_random_seed(25)

# Split data
X_eda = df_eda['eda']
y_eda = df_eda['label']

# Split dataset
X_eda_train, X_eda_val, y_eda_train, y_eda_val = train_test_split(
    X_eda, y_eda, test_size=0.3, random_state=25, stratify=y_eda
)

X_eda_val, X_eda_test, y_eda_val, y_eda_test = train_test_split(
    X_eda_val, y_eda_val, test_size=0.5, random_state=25, stratify=y_eda_val
)

print(f"Training set size: {X_eda_train.shape[0]}")
print(f"Validation set size: {X_eda_val.shape[0]}")
print(f"Testing set size: {X_eda_test.shape[0]}")

from transformers import AutoTokenizer

# Inisialisasi tokenizer
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

# Tokenisasi data
def tokenize_function(texts):
    return tokenizer(
        texts.tolist(),
        padding=True,
        truncation=True,
        max_length=100
    )

train_eda_encodings = tokenize_function(X_eda_train)
val_eda_encodings = tokenize_function(X_eda_val)
test_eda_encodings = tokenize_function(X_eda_test)

from torch.utils.data import Dataset # Import Dataset from torch.utils.data
import torch

class ReviewsDataset(Dataset):
    def __init__(self, encodings, label):
        self.encodings = encodings
        self.label = label

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        # Check if idx is a list (for batching) or an integer (single item)
        if isinstance(idx, list):
            # If idx is a list, create a batch of items
            item = {key: torch.tensor([val[i] for i in idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor([self.label[i] for i in idx])
        else:
            # If idx is an integer, get a single item
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['label'] = torch.tensor(self.label[idx])
        return item

train_eda_dataset = ReviewsDataset(train_eda_encodings, y_eda_train.tolist())
val_eda_dataset = ReviewsDataset(val_eda_encodings, y_eda_val.tolist())
test_eda_dataset = ReviewsDataset(test_eda_encodings, y_eda_test.tolist())

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, get_scheduler, BertConfig
from torch.optim import AdamW
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Konfigurasi model
config = BertConfig.from_pretrained(
    "indobenchmark/indobert-base-p1",
    num_labels=2,
    seed=25
)

model = AutoModelForSequenceClassification.from_pretrained(
    "indobenchmark/indobert-base-p1",
    config=config
)

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# Parameter training
train_batch_size = 32
num_train_epochs = 5
train_dataset_size = len(train_dataset)
total_training_steps = (train_dataset_size // train_batch_size) * num_train_epochs

# Scheduler LR
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=500,
    num_training_steps=total_training_steps
)

# Fungsi evaluasi
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# TrainingArguments dengan evaluasi per step dan save per step
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-5,

    # Penting: evaluasi dan save per step, agar load_best_model_at_end bisa jalan
    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=10,
    save_steps=10,

    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True,

    # Optional: untuk menghindari terlalu banyak checkpoint
    save_total_limit=3
)

# Inisialisasi Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_eda_dataset,
    eval_dataset=val_eda_dataset,
    compute_metrics=compute_metrics,
    optimizers=(optimizer, lr_scheduler)
)

# Train model
trainer.train()

import matplotlib.pyplot as plt

log_history = trainer.state.log_history

# List untuk menyimpan data
train_steps = []
train_loss = []

eval_steps = []
eval_loss = []
eval_accuracy = []

# Ekstrak data dari log_history
for log in log_history:
    # Training loss biasanya muncul saat step training
    if 'loss' in log and 'step' in log:
        train_steps.append(log['step'])
        train_loss.append(log['loss'])
    # Evaluation metrics muncul saat evaluasi
    if 'eval_loss' in log and 'step' in log:
        eval_steps.append(log['step'])
        eval_loss.append(log['eval_loss'])
    if 'eval_accuracy' in log and 'step' in log:
        eval_accuracy.append(log['eval_accuracy'])

# Plotting
plt.figure(figsize=(12, 5))

# Plot Loss (Training & Validation)
plt.subplot(1, 2, 1)
plt.plot(train_steps, train_loss, label='Training Loss')
plt.plot(eval_steps, eval_loss, label='Validation Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()

# Plot Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(eval_steps, eval_accuracy, label='Validation Accuracy', color='green')
plt.xlabel('Step')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Prediksi pada Data Testing
predictions = trainer.predict(test_eda_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids

# Menghitung dan Menampilkan Test Accuracy
test_accuracy = accuracy_score(y_true, y_pred)
print(f"Test Accuracy: {test_accuracy}")

# Classification Report

cr = classification_report(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print("=== Classification Report ===")
print(cr)
print("=== Confusion Matrix (===")
print(cm)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = ['Negative', 'Positive']
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()